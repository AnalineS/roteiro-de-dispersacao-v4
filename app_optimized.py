from flask import Flask, request, jsonify, render_template, Response, render_template_string
from flask_cors import CORS
import os
from transformers.pipelines import pipeline
import torch
import re
import logging
from datetime import datetime
import random
import requests
from sentence_transformers import SentenceTransformer
import numpy as np
import hashlib
import json
import base64

app = Flask(__name__)
CORS(app)

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Vari√°veis globais
MD_PATH = 'PDFs/Roteiro de Dsispensa√ß√£o - Hansen√≠ase.md'
md_text = ""
qa_pipeline = None
text_generation_pipeline = None
sentiment_pipeline = None
tokenizer = None
model = None
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
response_cache = {}

# Tr√™s chaves e modelos
OPENROUTER_API_KEY_LLAMA = os.environ.get("OPENROUTER_API_KEY_LLAMA", "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")
OPENROUTER_API_KEY_QWEN = os.environ.get("OPENROUTER_API_KEY_QWEN", "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")
OPENROUTER_API_KEY_GEMINI = os.environ.get("OPENROUTER_API_KEY_GEMINI", "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")
LLAMA3_MODEL = "meta-llama/llama-3.3-70b-instruct:free"
QWEN_MODEL = "qwen/qwen3-14b:free"
GEMINI_MODEL = "google/gemini-2.0-flash-exp:free"

# Templates de linguagem natural para cada persona
NATURAL_TEMPLATES = {
    "dr_gasnelio": {
        "greeting": [
            "Sauda√ß√µes! Sou o Dr. Gasnelio. Minha pesquisa foca no roteiro de dispensa√ß√£o para a pr√°tica da farm√°cia cl√≠nica. Como posso auxili√°-lo hoje?",
            "Ol√°! Aqui √© o Dr. Gasnelio. Tenho dedicado minha carreira ao estudo da dispensa√ß√£o farmac√™utica. Em que posso ajud√°-lo?",
            "Bem-vindo! Sou o Dr. Gasnelio, especialista em farm√°cia cl√≠nica. Como posso contribuir com sua consulta hoje?"
        ],
        "thinking": [
            "Deixe-me analisar essa quest√£o com base na minha pesquisa...",
            "Interessante pergunta. Vou consultar os dados da tese...",
            "Essa √© uma quest√£o importante. Permita-me buscar nas fontes..."
        ],
        "confidence_high": [
            "Baseado na minha pesquisa, posso afirmar com confian√ßa que:",
            "Os dados da tese s√£o bastante claros sobre isso:",
            "Minha an√°lise da literatura confirma que:"
        ],
        "confidence_medium": [
            "Com base no que encontrei na tese, posso sugerir que:",
            "Os dados dispon√≠veis indicam que:",
            "Baseado na pesquisa, parece que:"
        ],
        "confidence_low": [
            "Essa quest√£o √© interessante, mas n√£o encontrei dados espec√≠ficos na tese. Sugiro consultar:",
            "N√£o tenho informa√ß√µes detalhadas sobre isso na pesquisa, mas posso orientar para:",
            "Essa √°rea n√£o foi coberta especificamente na tese, mas posso sugerir:"
        ]
    },
    "ga": {
        "greeting": [
            "Opa, tudo certo? Aqui √© o G√°! T√¥ aqui pra gente desenrolar qualquer d√∫vida sobre o uso correto de medicamentos e o roteiro de dispensa√ß√£o. Manda a ver!",
            "E a√≠, beleza? Sou o G√°! T√¥ aqui pra te ajudar com qualquer parada sobre rem√©dios e farm√°cia. Fala a√≠!",
            "Oi! Aqui √© o G√°! T√¥ aqui pra gente conversar sobre medicamentos e como usar direitinho. Qual √© a boa?"
        ],
        "thinking": [
            "Deixa eu dar uma olhada na tese aqui...",
            "Hmm, deixa eu ver o que tem sobre isso...",
            "Vou procurar essa informa√ß√£o pra voc√™..."
        ],
        "confidence_high": [
            "Olha s√≥, encontrei isso na tese:",
            "T√° aqui, direto da pesquisa:",
            "D√° uma olhada nisso que achei:"
        ],
        "confidence_medium": [
            "Olha, pelo que vi na tese:",
            "Acho que √© mais ou menos assim:",
            "Pelo que entendi da pesquisa:"
        ],
        "confidence_low": [
            "Ih, essa eu n√£o sei certinho, mas posso te ajudar a procurar!",
            "N√£o achei essa informa√ß√£o espec√≠fica, mas posso te orientar!",
            "Essa parte n√£o t√° muito clara na tese, mas vamos ver o que tem!"
        ]
    }
}

# Caminho do arquivo de sin√¥nimos externo
SYNONYM_PATH = 'synonyms.json'

# Dicion√°rio padr√£o embutido
DEFAULT_SYNONYMS = {
    'hansen√≠ase': ['lepra', 'doen√ßa de hansen', 'mycobacterium leprae'],
    'medicamento': ['f√°rmaco', 'droga', 'rem√©dio', 'medica√ß√£o'],
    'dispensa√ß√£o': ['dispensar', 'entrega', 'fornecimento', 'distribui√ß√£o'],
    # ... outros termos ...
}

# Carrega sin√¥nimos do arquivo externo ou usa padr√£o

def load_synonyms():
    try:
        with open(SYNONYM_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.warning(f'N√£o foi poss√≠vel carregar {SYNONYM_PATH}: {e}. Usando dicion√°rio padr√£o.')
        return DEFAULT_SYNONYMS.copy()

synonyms = load_synonyms()

# Fun√ß√£o para recarregar manualmente (pode ser chamada por endpoint futuro)
def reload_synonyms():
    global synonyms
    synonyms = load_synonyms()
    logger.info('Dicion√°rio de sin√¥nimos recarregado.')

def expand_query_with_synonyms(question):
    expanded_terms = [question.lower()]
    for term, syns in synonyms.items():
        if term in question.lower():
            for synonym in syns:
                expanded_question = question.lower().replace(term, synonym)
                if expanded_question not in expanded_terms:
                    expanded_terms.append(expanded_question)
    return expanded_terms

def extract_md_text(md_path):
    """Extrai texto do arquivo Markdown"""
    global md_text
    try:
        with open(md_path, "r", encoding="utf-8") as file:
            text = file.read()
        logger.info(f"Arquivo Markdown extra√≠do com sucesso. Total de caracteres: {len(text)}")
        return text
    except Exception as e:
        logger.error(f"Erro ao extrair arquivo Markdown: {e}")
        return ""

def load_ai_models():
    """Carrega m√∫ltiplos modelos de IA gratuitos do Hugging Face"""
    global qa_pipeline, text_generation_pipeline, sentiment_pipeline, tokenizer, model
    try:
        # Modelo principal para QA (mais robusto)
        model_name = "deepset/roberta-base-squad2"
        logger.info(f"Carregando modelo QA: {model_name}")
        
        qa_pipeline = pipeline(
            "question-answering",
            model=model_name,
            tokenizer=model_name,
            device=-1 if not torch.cuda.is_available() else 0
        )
        
        # Modelo para gera√ß√£o de texto (mais natural)
        generation_model = "microsoft/DialoGPT-medium"
        logger.info(f"Carregando modelo de gera√ß√£o: {generation_model}")
        
        text_generation_pipeline = pipeline(
            "text-generation",
            model=generation_model,
            tokenizer=generation_model,
            device=-1 if not torch.cuda.is_available() else 0,
            max_length=100,
            do_sample=True,
            temperature=0.7
        )
        
        # Modelo para an√°lise de sentimento/contexto (comentado temporariamente)
        # sentiment_model = "cardiffnlp/twitter-roberta-base-sentiment"
        # logger.info(f"Carregando modelo de sentimento: {sentiment_model}")
        
        # sentiment_pipeline = pipeline(
        #     "sentiment-analysis",
        #     model=sentiment_model,
        #     device=-1 if not torch.cuda.is_available() else 0
        # )
        sentiment_pipeline = None
        
        logger.info("Todos os modelos carregados com sucesso")
    except Exception as e:
        logger.error(f"Erro ao carregar modelos: {e}")
        # Fallback para modelo √∫nico se houver erro
        try:
            qa_pipeline = pipeline(
                "question-answering",
                model="deepset/roberta-base-squad2",
                device=-1 if not torch.cuda.is_available() else 0
            )
        except Exception as e2:
            logger.error(f"Erro no fallback: {e2}")
            qa_pipeline = None

def get_natural_phrase(persona, category, confidence_level="medium"):
    """Retorna uma frase natural baseada na persona e contexto"""
    templates = NATURAL_TEMPLATES.get(persona, {})
    category_templates = templates.get(category, [])
    
    if category == "confidence":
        confidence_key = f"confidence_{confidence_level}"
        category_templates = templates.get(confidence_key, [])
    
    if category_templates:
        return random.choice(category_templates)
    return ""

def find_relevant_context_enhanced(question, full_text, max_length=800):
    """Encontra contexto mais relevante usando m√∫ltiplas estrat√©gias"""
    # Divide o texto em chunks menores com overlap
    chunks = []
    chunk_size = 1000  # Aumentado para pegar mais contexto
    overlap = 300      # Aumentado o overlap
    
    for i in range(0, len(full_text), chunk_size - overlap):
        chunk = full_text[i:i + chunk_size]
        if chunk.strip():
            chunks.append(chunk)
    
    if len(chunks) <= 2:
        return full_text[:max_length]
    
    # Busca por palavras-chave na pergunta
    question_words = set(re.findall(r'\w+', question.lower()))
    
    chunk_scores = []
    
    for chunk in chunks:
        chunk_words = set(re.findall(r'\w+', chunk.lower()))
        common_words = question_words.intersection(chunk_words)
        score = len(common_words) / len(question_words) if question_words else 0
        
        # B√¥nus para chunks que cont√™m termos m√©dicos espec√≠ficos
        medical_terms = ['medicamento', 'dose', 'tratamento', 'rea√ß√£o', 'efeito', 'hansen√≠ase', 'clofazimina', 'rifampicina', 'dapsona', 'acompanhamento', 'dispensa√ß√£o', 'paciente']
        for term in medical_terms:
            if term in chunk.lower():
                score += 0.1
        
        chunk_scores.append((chunk, score))
    
    # Ordena por score e pega os melhores
    chunk_scores.sort(key=lambda x: x[1], reverse=True)
    
    # Combina os 2 melhores chunks
    combined_context = ""
    for chunk, score in chunk_scores[:2]:
        if score > 0.05:  # Threshold m√≠nimo
            combined_context += chunk + "\n\n"
    
    return combined_context[:max_length] if combined_context else chunks[0][:max_length]

def enhance_response_with_generation(base_answer, question, persona):
    """Melhora a resposta usando gera√ß√£o de texto natural"""
    if not text_generation_pipeline or not base_answer:
        return base_answer
    
    try:
        # Cria um prompt contextual
        if persona == "dr_gasnelio":
            prompt = f"Como um farmac√™utico especialista, responda de forma t√©cnica mas acess√≠vel: {question} Resposta base: {base_answer}"
        else:
            prompt = f"Como um farmac√™utico amig√°vel, explique de forma simples: {question} Resposta base: {base_answer}"
        
        # Gera texto complementar
        generated = text_generation_pipeline(
            prompt,
            max_length=len(prompt.split()) + 20,
            do_sample=True,
            temperature=0.6
        )
        
        if generated and len(generated) > 0:
            enhanced_text = generated[0]['generated_text']
            # Extrai apenas a parte gerada (remove o prompt)
            if len(enhanced_text) > len(prompt):
                new_content = enhanced_text[len(prompt):].strip()
                if new_content:
                    return f"{base_answer}\n\n{new_content}"
        
        return base_answer
    except Exception as e:
        logger.error(f"Erro na gera√ß√£o de texto: {e}")
        return base_answer

def answer_question_optimized(question, persona, conversation_history=None):
    cache_key = f"{persona}_{hashlib.md5(question.encode()).hexdigest()}"
    if cache_key in response_cache:
        return response_cache[cache_key]
    
    global qa_pipeline, md_text
    
    if not qa_pipeline or not md_text:
        resposta = enhanced_fallback_response(question, persona, "")
    else:
        try:
            # Encontra contexto relevante
            context = find_relevant_context_enhanced(question, md_text)
            
            # Faz a pergunta ao modelo QA
            result = qa_pipeline(
                question=question,
                context=context,
                max_answer_len=300,
                handle_impossible_answer=True
            )
            
            # Acessa os resultados de forma segura
            if isinstance(result, dict):
                answer = result.get('answer', '').strip()
                confidence = result.get('score', 0.0)
            else:
                answer = ''
                confidence = 0.0
            
            logger.info(f"Pergunta: {question}")
            logger.info(f"Confian√ßa QA: {confidence}")
            logger.info(f"Resposta base: {answer}")
            
            # Determina o n√≠vel de confian√ßa
            if confidence > 0.6:
                confidence_level = "high"
            elif confidence > 0.3:
                confidence_level = "medium"
            else:
                confidence_level = "low"
            
            # Se a confian√ßa for baixa ou resposta vazia, usa fallback aprimorado
            if not answer or confidence < 0.2:
                logger.info("Usando fallback aprimorado - confian√ßa baixa")
                resposta = enhanced_fallback_response(question, persona, context)
            else:
                # Melhora a resposta com gera√ß√£o de texto
                enhanced_answer = enhance_response_with_generation(answer, question, persona)
                
                # Formata com linguagem natural
                resposta = format_persona_answer_enhanced(enhanced_answer, persona, confidence_level)
        except Exception as e:
            logger.error(f"Erro ao processar pergunta: {e}")
            resposta = enhanced_fallback_response(question, persona, "")
    
    response_cache[cache_key] = resposta
    return resposta

def format_persona_answer_enhanced(answer, persona, confidence_level):
    """Formata a resposta com linguagem natural aprimorada"""
    
    # Pega frase de introdu√ß√£o baseada na confian√ßa
    intro_phrase = get_natural_phrase(persona, "confidence", confidence_level)
    
    if persona == "dr_gasnelio":
        return {
            "answer": (
                f"Dr. Gasnelio responde:\n\n"
                f"{intro_phrase}\n\n"
                f"{answer}\n\n"
                f"*Baseado na minha tese sobre roteiro de dispensa√ß√£o para hansen√≠ase. "
                f"N√≠vel de confian√ßa: {'Alto' if confidence_level == 'high' else 'M√©dio' if confidence_level == 'medium' else 'Baixo'}.*"
            ),
            "persona": "dr_gasnelio",
            "confidence": confidence_level
        }
    elif persona == "ga":
        # Transforma completamente a resposta para o G√° - mais descontra√≠da e explicativa
        simple_answer = transform_for_ga(answer)
        return {
            "answer": (
                f"G√° responde:\n\n"
                f"{intro_phrase}\n\n"
                f"{simple_answer}\n\n"
                f"*T√° na tese, pode confiar! üòä*"
            ),
            "persona": "ga",
            "confidence": confidence_level
        }
    else:
        return {
            "answer": answer,
            "persona": "default",
            "confidence": confidence_level
        }

def transform_for_ga(text):
    """Transforma o texto t√©cnico em linguagem descontra√≠da e explicativa para o G√°"""
    # Remove aspas e formata√ß√µes t√©cnicas
    text = text.replace('"', '').replace('"', '').replace('"', '')
    
    # Simplifica termos t√©cnicos
    replacements = {
        "dispensa√ß√£o": "entrega do rem√©dio na farm√°cia",
        "medicamentos": "rem√©dios",
        "posologia": "como tomar o rem√©dio",
        "administra√ß√£o": "como usar o rem√©dio",
        "rea√ß√£o adversa": "efeito colateral",
        "intera√ß√£o medicamentosa": "mistura de rem√©dios que pode dar problema",
        "protocolo": "guia de cuidados",
        "orienta√ß√£o": "explica√ß√£o",
        "ades√£o": "seguir direitinho o tratamento",
        "tratamento": "tratamento",
        "hansen√≠ase": "hansen√≠ase",
        "paciente": "pessoa que est√° tratando",
        "supervisionada": "com algu√©m olhando junto",
        "autoadministrada": "a pr√≥pria pessoa toma sozinha",
        "prescri√ß√£o": "receita do m√©dico",
        "dose": "quantidade do rem√©dio",
        "contraindica√ß√£o": "quando n√£o pode usar",
        "indica√ß√£o": "quando √© recomendado usar",
        "poliquimioterapia": "mistura de rem√©dios",
        "rifampicina": "rifampicina",
        "clofazimina": "clofazimina",
        "dapsona": "dapsona",
        "mg": "miligramas",
        "dose mensal": "rem√©dio que toma uma vez por m√™s",
        "dose di√°ria": "rem√©dio que toma todo dia",
        "supervisionada": "com algu√©m olhando",
        "autoadministrada": "a pessoa toma sozinha"
    }
    
    # Aplica as substitui√ß√µes
    for technical, simple in replacements.items():
        text = text.replace(technical, simple)
    
    # Adiciona express√µes descontra√≠das
    casual_expressions = [
        "Olha s√≥, ",
        "Ent√£o, ",
        "Tipo assim, ",
        "Sabe como √©, ",
        "√â o seguinte, ",
        "Fica ligado, ",
        "Cara, ",
        "Mano, ",
        "Beleza, ",
        "Tranquilo, "
    ]
    
    # Quebra o texto em frases e adiciona express√µes casuais
    sentences = text.split('. ')
    if sentences and len(sentences) > 0:
        first_sentence = sentences[0]
        if not any(expr in first_sentence for expr in casual_expressions):
            sentences[0] = random.choice(casual_expressions) + first_sentence.lower()
    
    # Reconstr√≥i o texto
    transformed_text = '. '.join(sentences)
    
    # Adiciona emojis e express√µes informais
    emoji_replacements = {
        "importante": "importante ‚ö†Ô∏è",
        "cuidado": "cuidado ‚ö†Ô∏è",
        "aten√ß√£o": "aten√ß√£o üëÄ",
        "lembre": "lembre üí°",
        "consulte": "consulte üë®‚Äç‚öïÔ∏è",
        "m√©dico": "m√©dico üë®‚Äç‚öïÔ∏è",
        "farmac√™utico": "farmac√™utico üíä",
        "rem√©dio": "rem√©dio üíä",
        "tratamento": "tratamento üè•"
    }
    
    for word, replacement in emoji_replacements.items():
        transformed_text = transformed_text.replace(word, replacement)
    
    return transformed_text

def enhanced_fallback_response(question, persona, context):
    """Fallback aprimorado que retorna trecho relevante do PDF"""
    logger.info("Executando fallback aprimorado")
    
    # Busca por palavras-chave na pergunta
    question_words = set(re.findall(r'\w+', question.lower()))
    
    # Divide o contexto em par√°grafos
    paragraphs = context.split('\n\n') if context else md_text.split('\n\n')
    
    best_paragraph = None
    best_score = 0.0
    
    for paragraph in paragraphs:
        if len(paragraph.strip()) < 50:  # Ignora par√°grafos muito pequenos
            continue
            
        paragraph_words = set(re.findall(r'\w+', paragraph.lower()))
        common_words = question_words.intersection(paragraph_words)
        score = len(common_words) / len(question_words) if question_words else 0
        
        if score > best_score:
            best_score = score
            best_paragraph = paragraph
    
    # Se encontrou um par√°grafo relevante
    if best_paragraph and best_score > 0.1:
        logger.info(f"Par√°grafo relevante encontrado com score: {best_score}")
        
        if persona == "dr_gasnelio":
            # Para o Dr. Gasnelio, retorna o par√°grafo completo sem cortes
            complete_paragraph = best_paragraph.strip()
            
            # Se o par√°grafo parece estar cortado, tenta encontrar o contexto completo
            if complete_paragraph.endswith('...') or len(complete_paragraph) < 100:
                # Busca por par√°grafos relacionados
                related_paragraphs = []
                for para in paragraphs:
                    if any(word in para.lower() for word in question_words):
                        related_paragraphs.append(para.strip())
                
                if related_paragraphs:
                    complete_paragraph = '\n\n'.join(related_paragraphs[:2])  # Pega at√© 2 par√°grafos relacionados
            
            return {
                "answer": (
                    f"Dr. Gasnelio responde:\n\n"
                    f"Baseado na minha tese sobre roteiro de dispensa√ß√£o para hansen√≠ase, encontrei esta informa√ß√£o t√©cnica relevante:\n\n"
                    f"\"{complete_paragraph}\"\n\n"
                    f"*Esta √© uma extra√ß√£o direta do texto da tese. Para informa√ß√µes mais espec√≠ficas, recomendo consultar a se√ß√£o completa.*"
                ),
                "persona": "dr_gasnelio",
                "confidence": "low"
            }
        elif persona == "ga":
            # Transforma completamente para o G√° - n√£o copia texto direto
            ga_explanation = explain_like_ga(best_paragraph.strip(), question)
            return {
                "answer": (
                    f"G√° responde:\n\n"
                    f"Olha s√≥, encontrei algumas informa√ß√µes sobre isso na tese! üòä\n\n"
                    f"{ga_explanation}\n\n"
                    f"*T√° na tese, pode confiar! Mas se tiver d√∫vida, √© s√≥ perguntar de novo! üòâ*"
                ),
                "persona": "ga",
                "confidence": "low"
            }
        else:
            return {
                "answer": (
                    f"Encontrei esta informa√ß√£o na tese:\n\n"
                    f"\"{best_paragraph.strip()}\""
                ),
                "persona": "default",
                "confidence": "low"
            }
    
    # Se n√£o encontrou nada relevante, usa fallback padr√£o
    logger.info("Nenhum par√°grafo relevante encontrado, usando fallback padr√£o")
    return fallback_response(persona, "Informa√ß√£o n√£o encontrada na tese")

def explain_like_ga(technical_text, question):
    """Explica o texto t√©cnico de forma descontra√≠da como o G√° faria"""
    
    # Identifica o tipo de pergunta para dar uma resposta mais contextual
    question_lower = question.lower()
    
    if "rea√ß√£o" in question_lower or "efeito" in question_lower or "colateral" in question_lower:
        return (
            "Cara, sobre efeitos colaterais, √© sempre bom ficar ligado! ‚ö†Ô∏è "
            "Os rem√©dios podem dar algumas rea√ß√µes, sabe? Tipo, pode dar dor de barriga, "
            "enjoo, ou at√© mudar a cor da pele. Mas n√£o se preocupa, isso √© normal! "
            "O importante √© sempre conversar com o m√©dico se sentir algo estranho. "
            "E lembra: cada pessoa reage diferente, ent√£o n√£o se compara com os outros! üòä"
        )
    
    elif "dose" in question_lower or "como tomar" in question_lower or "posologia" in question_lower:
        return (
            "Beleza, sobre como tomar os rem√©dios! üíä "
            "Tem alguns que voc√™ toma todo dia, outros que toma uma vez por m√™s com algu√©m olhando. "
            "√â tipo assim: alguns s√£o pra tomar com comida, outros n√£o podem tomar com suco de laranja. "
            "O importante √© seguir direitinho o que o m√©dico passou! "
            "Se esquecer de tomar, n√£o se desespera, s√≥ toma quando lembrar. "
            "Mas se estiver muito perto da pr√≥xima dose, pula a que esqueceu! üòâ"
        )
    
    elif "acompanhamento" in question_lower or "seguimento" in question_lower:
        return (
            "Ent√£o, sobre o acompanhamento! üë®‚Äç‚öïÔ∏è "
            "√â super importante ir nas consultas direitinho, sabe? "
            "O m√©dico vai ficar de olho pra ver se est√° tudo funcionando bem. "
            "Tem que fazer alguns exames tamb√©m, pra garantir que est√° tudo certo. "
            "E se sentir qualquer coisa estranha, corre falar com eles! "
            "N√£o fica com vergonha de perguntar, eles est√£o l√° pra isso! üòä"
        )
    
    elif "hansen√≠ase" in question_lower or "tratamento" in question_lower:
        return (
            "Cara, sobre o tratamento da hansen√≠ase! üè• "
            "√â um tratamento que demora um pouco, mas funciona super bem! "
            "Voc√™ vai tomar alguns rem√©dios juntos, tipo uma mistura. "
            "Alguns s√£o pra tomar todo dia, outros uma vez por m√™s. "
            "O importante √© n√£o parar no meio, mesmo que melhore! "
            "Se parar, pode voltar pior. Ent√£o firmeza, vamo at√© o final! üí™"
        )
    
    else:
        # Resposta gen√©rica mas descontra√≠da
        return (
            "Olha s√≥, encontrei algumas informa√ß√µes legais sobre isso! üí° "
            "√â sempre bom ficar ligado nos detalhes, sabe? "
            "Cada coisa tem seu jeito certo de fazer. "
            "Se tiver d√∫vida sobre algo espec√≠fico, √© s√≥ perguntar! "
            "T√¥ aqui pra ajudar! üòä"
        )

def fallback_response(persona, reason=""):
    """Resposta de fallback quando n√£o encontra informa√ß√£o"""
    logger.info(f"Executando fallback padr√£o para persona: {persona}")
    
    if persona == "dr_gasnelio":
        return {
            "answer": (
                f"Dr. Gasnelio responde:\n\n"
                f"N√£o encontrei uma resposta exata na tese para sua pergunta. Recomendo consultar as se√ß√µes de Seguran√ßa, Rea√ß√µes Adversas ou Intera√ß√µes para mais detalhes.\n\n"
                f"Se quiser, posso tentar explicar de outra forma ou buscar em outra parte do texto. Fique √† vontade para perguntar novamente! {reason}"
            ),
            "persona": "dr_gasnelio",
            "confidence": "low"
        }
    elif persona == "ga":
        return {
            "answer": (
                f"G√° responde:\n\n"
                f"Ih, n√£o achei uma resposta certinha na tese, mas posso te ajudar a procurar! D√° uma olhada nas partes de efeitos colaterais ou seguran√ßa, ou me pergunte de outro jeito que eu tento de novo! {reason}"
            ),
            "persona": "ga",
            "confidence": "low"
        }
    else:
        return {
            "answer": (
                f"Desculpe, n√£o encontrei essa informa√ß√£o na tese. {reason}"
            ),
            "persona": "default",
            "confidence": "low"
        }

def call_openrouter_model(question, context, persona, model, api_key):
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "http://localhost:5000",
        "X-Title": "Chatbot Tese Hanseniase"
    }
    if persona == "dr_gasnelio":
        system_prompt = (
            "Voc√™ √© o Dr. Gasnelio, farmac√™utico pesquisador, responde de forma t√©cnica, formal e baseada em evid√™ncias. Use o contexto abaixo para responder de forma precisa e objetiva."
        )
    else:
        system_prompt = (
            "Voc√™ √© a G√°, uma assistente amig√°vel e did√°tica. Explique de forma simples, acess√≠vel e acolhedora, usando o contexto abaixo."
        )
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": f"{system_prompt}\n\nContexto:\n{context}"},
            {"role": "user", "content": question}
        ]
    }
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=60)
        if response.status_code != 200:
            raise Exception(f"Erro OpenRouter: {response.status_code} - {response.text}")
        data = response.json()
        return data['choices'][0]['message']['content']
    except Exception as e:
        print(f"Erro ao chamar OpenRouter ({model}): {e}\nResposta: {getattr(e, 'response', None)}")
        return None

# Fun√ß√£o principal com fallback (Llama -> Qwen -> Gemini)
def call_chatbot_with_fallback(question, context, persona):
    # 1. Llama
    resposta = call_openrouter_model(question, context, persona, LLAMA3_MODEL, OPENROUTER_API_KEY_LLAMA)
    if resposta:
        return resposta
    # 2. Qwen
    resposta = call_openrouter_model(question, context, persona, QWEN_MODEL, OPENROUTER_API_KEY_QWEN)
    if resposta:
        return resposta
    # 3. Gemini
    resposta = call_openrouter_model(question, context, persona, GEMINI_MODEL, OPENROUTER_API_KEY_GEMINI)
    if resposta:
        return resposta
    return "[Erro ao consultar os modelos OpenRouter. Por favor, tente novamente mais tarde.]"

# Senha de administra√ß√£o (N√ÉO DEIXAR APARENTE NO C√ìDIGO)
ADMIN_PASSWORD = os.environ.get('ADMIN_PASSWORD', None) or ''.join([chr(int(x)) for x in ['56','114','49','115','116','48','108']])  # "8r1st0l"

# Fun√ß√£o utilit√°ria para autentica√ß√£o b√°sica

def check_auth(auth_header):
    if not auth_header or not auth_header.startswith('Basic '):
        return False
    try:
        auth_decoded = base64.b64decode(auth_header.split(' ')[1]).decode('utf-8')
        username, password = auth_decoded.split(':', 1)
        return password == ADMIN_PASSWORD
    except Exception:
        return False

def require_admin_auth():
    auth = request.headers.get('Authorization')
    if not check_auth(auth):
        return Response('Acesso restrito', 401, {'WWW-Authenticate': 'Basic realm="Admin"'})

# Endpoint do painel administrativo de sin√¥nimos
@app.route('/admin/synonyms', methods=['GET'])
def admin_synonyms():
    resp = require_admin_auth()
    if resp: return resp
    # Serve HTML diretamente (pode ser movido para template)
    html = '''
    <!DOCTYPE html>
    <html lang="pt-BR">
    <head>
      <meta charset="UTF-8">
      <title>Painel de Sin√¥nimos</title>
      <style>
        body { font-family: sans-serif; background: #f8f9fa; margin: 0; padding: 2em; }
        table { border-collapse: collapse; width: 100%; background: #fff; }
        th, td { border: 1px solid #ddd; padding: 8px; }
        th { background: #eee; }
        input, button { padding: 6px; }
        .actions { display: flex; gap: 8px; }
        .success { color: green; }
        .error { color: red; }
      </style>
    </head>
    <body>
      <h2>Administra√ß√£o de Sin√¥nimos</h2>
      <div id="feedback"></div>
      <table id="synonyms-table">
        <thead>
          <tr>
            <th>Termo</th>
            <th>Sin√¥nimos (separados por v√≠rgula)</th>
            <th>A√ß√µes</th>
          </tr>
        </thead>
        <tbody></tbody>
      </table>
      <button onclick="addRow()">Adicionar termo</button>
      <button onclick="saveSynonyms()">Salvar altera√ß√µes</button>
      <button onclick="reloadSynonyms()">Recarregar dicion√°rio</button>
      <script>
        let synonyms = {};
        const tableBody = document.querySelector('#synonyms-table tbody');
        const feedback = document.getElementById('feedback');
        function renderTable() {
          tableBody.innerHTML = '';
          Object.entries(synonyms).forEach(([term, syns], idx) => {
            const row = document.createElement('tr');
            row.innerHTML = `
              <td><input value="${term}" data-idx="${idx}" class="term-input"></td>
              <td><input value="${syns.join(', ')}" data-idx="${idx}" class="syns-input"></td>
              <td class="actions">
                <button onclick="removeRow(${idx})">Remover</button>
              </td>
            `;
            tableBody.appendChild(row);
          });
        }
        function addRow() {
          synonyms[''] = [];
          renderTable();
        }
        function removeRow(idx) {
          const keys = Object.keys(synonyms);
          delete synonyms[keys[idx]];
          renderTable();
        }
        async function fetchSynonyms() {
          const res = await fetch('/static/synonyms.json');
          synonyms = await res.json();
          renderTable();
        }
        async function saveSynonyms() {
          // Coleta dados da tabela
          const rows = tableBody.querySelectorAll('tr');
          const newSynonyms = {};
          rows.forEach(row => {
            const term = row.querySelector('.term-input').value.trim();
            const syns = row.querySelector('.syns-input').value.split(',').map(s => s.trim()).filter(Boolean);
            if (term) newSynonyms[term] = syns;
          });
          // Envia para o backend
          const res = await fetch('/api/update_synonyms', {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify(newSynonyms)
          });
          const data = await res.json();
          feedback.textContent = data.message;
          feedback.className = data.status === 'success' ? 'success' : 'error';
        }
        async function reloadSynonyms() {
          const res = await fetch('/api/reload_synonyms', {method: 'POST'});
          const data = await res.json();
          feedback.textContent = data.message;
          feedback.className = data.status === 'success' ? 'success' : 'error';
        }
        fetchSynonyms();
      </script>
    </body>
    </html>
    '''
    return render_template_string(html)

# Endpoint para atualizar o arquivo synonyms.json
@app.route('/api/update_synonyms', methods=['POST'])
def api_update_synonyms():
    resp = require_admin_auth()
    if resp: return resp
    try:
        new_synonyms = request.get_json(force=True)
        with open(SYNONYM_PATH, 'w', encoding='utf-8') as f:
            json.dump(new_synonyms, f, ensure_ascii=False, indent=2)
        return jsonify({"status": "success", "message": "Sin√¥nimos atualizados com sucesso. Clique em 'Recarregar dicion√°rio' para aplicar."})
    except Exception as e:
        logger.error(f"Erro ao atualizar sin√¥nimos: {e}")
        return jsonify({"status": "error", "message": f"Erro ao atualizar sin√¥nimos: {e}"}), 500

@app.route('/')
def index():
    """P√°gina principal"""
    return render_template('index.html')

@app.route('/tese')
def tese():
    return render_template('tese.html')

@app.route('/script.js')
def serve_script():
    """Serve o arquivo script.js na raiz do projeto"""
    return app.send_static_file('script.js')

@app.route('/test')
def test_page():
    """P√°gina de teste"""
    return render_template('test_js.html')

ANALYTICS_PATH = 'analytics.json'

def load_analytics():
    try:
        with open(ANALYTICS_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return {
            'total': 0,
            'por_persona': {},
            'por_data': {},
            'sem_resposta': 0
        }

def save_analytics(data):
    with open(ANALYTICS_PATH, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# Atualiza analytics a cada pergunta

def update_analytics(persona, had_answer):
    from datetime import date
    analytics = load_analytics()
    analytics['total'] = analytics.get('total', 0) + 1
    analytics['por_persona'][persona] = analytics['por_persona'].get(persona, 0) + 1
    today = str(date.today())
    analytics['por_data'][today] = analytics['por_data'].get(today, 0) + 1
    if not had_answer:
        analytics['sem_resposta'] = analytics.get('sem_resposta', 0) + 1
    save_analytics(analytics)

# Modifique o endpoint /api/chat para atualizar analytics
@app.route('/api/chat', methods=['POST'])
def chat_api():
    try:
        if not request.is_json:
            return jsonify({"error": "Requisi√ß√£o deve ser JSON"}), 400
        data = request.get_json(force=True, silent=True)
        if not data:
            return jsonify({"error": "JSON inv√°lido ou vazio"}), 400
        question = data.get('question', '').strip()
        personality_id = data.get('personality_id', 'dr_gasnelio')
        if not question:
            return jsonify({"error": "Pergunta n√£o fornecida"}), 400
        if personality_id not in ['dr_gasnelio', 'ga']:
            return jsonify({"error": "Personalidade inv√°lida"}), 400
        resposta = answer_question_optimized(question, personality_id, None)
        # Atualiza analytics
        had_answer = isinstance(resposta, dict) and resposta.get('answer') and resposta.get('answer').strip() != ''
        update_analytics(personality_id, had_answer)
        return jsonify({"answer": resposta})
    except Exception as e:
        logger.error(f"Erro na API de chat: {e}")
        return jsonify({"error": "Erro interno do servidor"}), 500

# Endpoint protegido para visualizar analytics
@app.route('/admin/analytics', methods=['GET'])
def admin_analytics():
    resp = require_admin_auth()
    if resp: return resp
    analytics = load_analytics()
    # KPIs
    total = analytics.get('total', 0)
    sem_resposta = analytics.get('sem_resposta', 0)
    por_persona = analytics.get('por_persona', {})
    por_data = analytics.get('por_data', {})
    dias = len(por_data)
    media_diaria = round(total / dias, 2) if dias else 0
    pct_sem_resposta = round(100 * sem_resposta / total, 1) if total else 0
    # Dados para gr√°ficos
    datas = list(sorted(por_data.keys()))
    valores_datas = [por_data[d] for d in datas]
    personas = list(por_persona.keys())
    valores_personas = [por_persona[p] for p in personas]
    html = f'''
    <html><head><title>Analytics do Chatbot</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>body{{font-family:sans-serif;background:#f8f9fa;padding:2em;}}.kpi{{display:inline-block;margin:1em 2em 1em 0;padding:1em 2em;background:#fff;border-radius:8px;box-shadow:0 2px 8px #0001;}}.kpi-title{{font-size:1em;color:#888;}}.kpi-value{{font-size:2em;font-weight:bold;}}.kpi-trend{{font-size:0.9em;color:#4ade80;}}.kpi-danger{{color:#f87171;}}.chart-container{{background:#fff;padding:1em;border-radius:8px;box-shadow:0 2px 8px #0001;margin-bottom:2em;}}</style></head><body>
    <h2>Analytics do Chatbot</h2>
    <div class="kpi"><div class="kpi-title">Total de perguntas</div><div class="kpi-value">{total}</div></div>
    <div class="kpi"><div class="kpi-title">% sem resposta</div><div class="kpi-value {'kpi-danger' if pct_sem_resposta > 10 else ''}">{pct_sem_resposta}%</div></div>
    <div class="kpi"><div class="kpi-title">M√©dia di√°ria</div><div class="kpi-value">{media_diaria}</div></div>
    <div class="kpi"><div class="kpi-title">Dias de uso</div><div class="kpi-value">{dias}</div></div>
    <div class="chart-container">
      <canvas id="lineChart" width="600" height="220"></canvas>
      <div style="text-align:center;font-size:0.95em;color:#666;margin-top:0.5em;">Tend√™ncia de perguntas por dia</div>
    </div>
    <div class="chart-container">
      <canvas id="pieChart" width="400" height="220"></canvas>
      <div style="text-align:center;font-size:0.95em;color:#666;margin-top:0.5em;">Distribui√ß√£o por persona</div>
    </div>
    <div style="max-width:700px;margin:2em auto 0 auto;background:#fff;padding:1.5em;border-radius:8px;box-shadow:0 2px 8px #0001;">
      <h3>Como interpretar os dados?</h3>
      <ul>
        <li><b>Total de perguntas</b>: volume geral de intera√ß√µes com o chatbot.</li>
        <li><b>% sem resposta</b>: indica perguntas que n√£o foram respondidas (quanto menor, melhor). Acima de 10% pode indicar necessidade de melhorar o conte√∫do ou os sin√¥nimos.</li>
        <li><b>M√©dia di√°ria</b>: uso m√©dio por dia desde o primeiro acesso.</li>
        <li><b>Tend√™ncia de perguntas por dia</b>: avalie se o uso est√° crescendo, est√°vel ou caindo.</li>
        <li><b>Distribui√ß√£o por persona</b>: mostra qual persona √© mais utilizada pelos usu√°rios.</li>
      </ul>
      <p style="color:#888;font-size:0.95em;">Esses indicadores ajudam a monitorar o engajamento, identificar oportunidades de melhoria e justificar o impacto do chatbot.</p>
    </div>
    <script>
      // Gr√°fico de linhas (perguntas por data)
      new Chart(document.getElementById('lineChart').getContext('2d'), {{
        type: 'line',
        data: {{
          labels: {datas},
          datasets: [{{
            label: 'Perguntas por dia',
            data: {valores_datas},
            borderColor: '#2563eb',
            backgroundColor: 'rgba(37,99,235,0.1)',
            fill: true,
            tension: 0.3
          }}]
        }},
        options: {{
          responsive: true,
          plugins: {{legend: {{display: false}}}},
          scales: {{y: {{beginAtZero: true}}}}
        }}
      }});
      // Gr√°fico de pizza (por persona)
      new Chart(document.getElementById('pieChart').getContext('2d'), {{
        type: 'pie',
        data: {{
          labels: {personas},
          datasets: [{{
            data: {valores_personas},
            backgroundColor: ['#fbbf24','#2563eb','#10b981','#f87171']
          }}]
        }},
        options: {{responsive: true}}
      }});
    </script>
    </body></html>
    '''
    return html

@app.route('/api/health', methods=['GET'])
def health_check():
    """Verifica√ß√£o de sa√∫de da API"""
    return jsonify({
        "status": "healthy",
        "qa_model_loaded": qa_pipeline is not None,
        "generation_model_loaded": text_generation_pipeline is not None,
        "sentiment_model_loaded": sentiment_pipeline is not None,
        "md_loaded": len(md_text) > 0,
        "embedding_model_loaded": embedding_model is not None,
        "response_cache_size": len(response_cache),
        "timestamp": datetime.now().isoformat()
    })

@app.route('/api/info', methods=['GET'])
def api_info():
    """Informa√ß√µes sobre a API"""
    return jsonify({
        "name": "Chatbot Tese Hansen√≠ase API - Vers√£o H√≠brida",
        "version": "3.0.0",
        "description": "API h√≠brida otimizada para chatbot baseado na tese sobre roteiro de dispensa√ß√£o para hansen√≠ase, combinando m√∫ltiplos modelos de IA gratuitos",
        "personas": {
            "dr_gasnelio": "Professor s√©rio e t√©cnico",
            "ga": "Amigo descontra√≠do que explica de forma simples"
        },
        "models": {
            "qa_model": "deepset/roberta-base-squad2",
            "generation_model": "microsoft/DialoGPT-medium",
            "sentiment_model": "cardiffnlp/twitter-roberta-base-sentiment"
        },
        "source": "Roteiro de Dispensa√ß√£o para Hansen√≠ase (Markdown)",
        "features": [
            "Sistema h√≠brido com m√∫ltiplos modelos de IA",
            "Linguagem natural contextual para ambas as personas",
            "Gera√ß√£o de texto complementar",
            "An√°lise de sentimento para contexto",
            "Fallback aprimorado com trechos relevantes",
            "Simplifica√ß√£o autom√°tica para o G√°",
            "Contexto inteligente para perguntas",
            "Cache de respostas",
            "Busca h√≠brida (palavras-chave + embeddings)",
            "Chunking aprimorado"
        ]
    })

@app.route('/api/reload_synonyms', methods=['POST'])
def api_reload_synonyms():
    """Endpoint para recarregar o dicion√°rio de sin√¥nimos sem reiniciar o app"""
    try:
        reload_synonyms()
        return jsonify({"status": "success", "message": "Dicion√°rio de sin√¥nimos recarregado com sucesso."})
    except Exception as e:
        logger.error(f"Erro ao recarregar sin√¥nimos: {e}")
        return jsonify({"status": "error", "message": f"Erro ao recarregar sin√¥nimos: {e}"}), 500

if __name__ == '__main__':
    # Inicializa√ß√£o
    logger.info("Iniciando aplica√ß√£o h√≠brida otimizada...")
    
    # Carrega o arquivo Markdown
    if os.path.exists(MD_PATH):
        md_text = extract_md_text(MD_PATH)
    else:
        logger.warning(f"Arquivo Markdown n√£o encontrado: {MD_PATH}")
        md_text = "Arquivo Markdown n√£o dispon√≠vel"
    
    # Carrega os modelos de IA
    load_ai_models()
    
    # Inicia o servidor
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('FLASK_ENV') == 'development'
    
    logger.info(f"Servidor h√≠brido otimizado iniciado na porta {port}")
    app.run(host='0.0.0.0', port=port, debug=debug) 